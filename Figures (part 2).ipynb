{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import slideflow as sf\n",
    "import torch\n",
    "import numpy as np\n",
    "from slideflow.io.torch import whc_to_cwh\n",
    "from PIL import Image\n",
    "from slideflow.mil.eval import run_inference\n",
    "from slideflow.mil.utils import load_model_weights\n",
    "from slideflow.model.extractors import rebuild_extractor\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "PROJECT_DIR = os.getcwd()\n",
    "\n",
    "PROJECT_DIR = '/mnt/data/fred/slideflow-gan'\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c8079",
   "metadata": {},
   "source": [
    "# MIL Model Feature Traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from slideflow.gan.stylegan3.stylegan3 import dnnlib, legacy, utils\n",
    "with dnnlib.util.open_url(PROJECT_DIR + '/FINAL_MODELS/CTransPath/snapshot.pkl') as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "\n",
    "    \n",
    "## Applies gradient descent to transition a 'base' feature vector towards a lower or higher prediction\n",
    "## for the outcome of interest from the MIL model.\n",
    "## Dataframe df_mod to provide a list of 'base' feature vectors from the dataset\n",
    "## model_path is a path to the model to evaluate transitions\n",
    "## model, config - are the loaded torch model\n",
    "## base_imgs - provides the img to select from df_mod to visualize\n",
    "def get_img_dict(df_mod, model_path, model, config, base_imgs, regen = True):\n",
    "    feat_cols = list(df_mod.columns.values)\n",
    "    feat_cols = [f for f in feat_cols if 'Feature_' in f]\n",
    "    if not regen and os.path.isfile(os.path.dirname(model_path) + \"/img_dict.pkl\"):\n",
    "        with open(os.path.dirname(model_path) + \"/img_dict.pkl\", 'rb') as f:\n",
    "            img_dict = pickle.load(f)\n",
    "            return img_dict\n",
    "        \n",
    "    img_dict = {}\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss().to(device) \n",
    "    for b in [base_imgs]:\n",
    "        vector_base = torch.tensor([df_mod[feat_cols].loc[b, :].values.tolist()]).to(device)\n",
    "        vector_base = torch.unsqueeze(vector_base, 0)\n",
    "        vector_base.requires_grad = True\n",
    "        lr = 2e-2\n",
    "\n",
    "        optimizer = torch.optim.Adam([vector_base], lr=lr)\n",
    "        imgs = []\n",
    "        for i in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            pred, _, _ = run_inference(model, vector_base, attention = True, use_lens = config.model_config.use_lens)\n",
    "            loss = loss_fn(pred[0][1], torch.tensor(0.0).to(device))\n",
    "            if i in [0, 20, 50, 199]: \n",
    "                img_gen = G(vector_base[0], 0, noise_mode ='const')\n",
    "                imgs += [((img_gen + 1)*127.5).permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()]     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        imgs = list(reversed(imgs))\n",
    "\n",
    "        vector_base = torch.tensor([df_mod[feat_cols].loc[b, :].values.tolist()]).to(device)\n",
    "        vector_base = torch.unsqueeze(vector_base, 0)\n",
    "        vector_base.requires_grad = True\n",
    "        optimizer = torch.optim.Adam([vector_base], lr=lr)\n",
    "\n",
    "\n",
    "        for i in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            pred, _, _ = run_inference(model, vector_base, attention = True, use_lens = config.model_config.use_lens)\n",
    "            loss = loss_fn(pred[0][1], torch.tensor(1.0).to(device))\n",
    "            if i in [20, 50, 199]:\n",
    "                img_gen = G(vector_base[0], 0, noise_mode ='const')\n",
    "                imgs += [((img_gen + 1)*127.5).permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        img_dict[b] = imgs\n",
    "    with open(os.path.dirname(model_path) + \"/img_dict.pkl\", 'wb') as f:\n",
    "        pickle.dump(img_dict[base_imgs], f)\n",
    "\n",
    "    return img_dict[base_imgs]\n",
    "\n",
    "## Applies gradient descent to transition a 'base' feature vector towards a lower or higher attention\n",
    "## for the MIL model.\n",
    "## Dataframe df_mod to provide a list of 'base' feature vectors from the dataset\n",
    "## model_path is a path to the model to evaluate transitions\n",
    "## model, config - are the loaded torch model\n",
    "## base_imgs - provides the img to select from df_mod to visualize\n",
    "\n",
    "def get_img_dict_attn(df_mod, model_path, model, config, base_imgs, regen = True):\n",
    "    feat_cols = list(df_mod.columns.values)\n",
    "    feat_cols = [f for f in feat_cols if 'Feature_' in f]\n",
    "    if not regen and os.path.isfile(os.path.dirname(model_path) + \"/img_dict_attn.pkl\"):\n",
    "        with open(os.path.dirname(model_path) + \"/img_dict_attn.pkl\", 'rb') as f:\n",
    "            img_dict = pickle.load(f)\n",
    "            return img_dict[base_imgs]\n",
    "        \n",
    "    img_dict = {}\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss().to(device) \n",
    "    for b in [base_imgs]:\n",
    "        vector_base = torch.tensor([df_mod[feat_cols].loc[b, :].values.tolist()]).to(device)\n",
    "        vector_base = torch.unsqueeze(vector_base, 0)\n",
    "        vector_base.requires_grad = True\n",
    "        lr = 3e-3\n",
    "\n",
    "        optimizer = torch.optim.Adam([vector_base], lr=lr)\n",
    "        imgs = []\n",
    "        for i in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            _, at, _ = run_inference(model, vector_base, attention = True, use_lens = config.model_config.use_lens)\n",
    "            loss =  1/(1 + torch.exp(-at))\n",
    "            if i in [0, 50, 100, 199]: \n",
    "                img_gen = G(vector_base[0], 0, noise_mode ='const')\n",
    "                imgs += [((img_gen + 1)*127.5).permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()]     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        imgs = list(reversed(imgs))\n",
    "\n",
    "        vector_base = torch.tensor([df_mod[feat_cols].loc[b, :].values.tolist()]).to(device)\n",
    "        vector_base = torch.unsqueeze(vector_base, 0)\n",
    "        vector_base.requires_grad = True\n",
    "        optimizer = torch.optim.Adam([vector_base], lr=lr)\n",
    "\n",
    "\n",
    "        for i in range(200):\n",
    "            optimizer.zero_grad()\n",
    "            _, at, _ = run_inference(model, vector_base, attention = True, use_lens = config.model_config.use_lens)\n",
    "            loss =  1/(1 + torch.exp(at))\n",
    "            if i in [50, 100, 199]:\n",
    "                img_gen = G(vector_base[0], 0, noise_mode ='const')\n",
    "                imgs += [((img_gen + 1)*127.5).permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        img_dict[b] = imgs\n",
    "    with open(os.path.dirname(model_path) + \"/img_dict_attn.pkl\", 'wb') as f:\n",
    "        pickle.dump(img_dict[base_imgs], f)\n",
    "\n",
    "    return img_dict[base_imgs]\n",
    "\n",
    "\n",
    "## Applies gradient descent to transition a 'base' feature vector towards a lower or higher prediction or attention\n",
    "## for the specified MIL model, and plots the resulting images.\n",
    "##\n",
    "## Dataframe df_mod to provide a list of 'base' feature vectors from the dataset\n",
    "## model_path is a path to the model to evaluate transitions\n",
    "## model, config - are the loaded torch model\n",
    "## base_imgs - provides the img to select from df_mod to visualize\n",
    "## regen - set to true to regenerate images, if false will use saved pickled images\n",
    "## datasets - provide the name of the datasets used for the models\n",
    "\n",
    "def plt_imgs(df_mod, model_path, model, config, base_imgs, regen, attn, subfig, label, datasets):\n",
    "    \n",
    "    rows = len(base_imgs)\n",
    "    img_dict = {}\n",
    "    if attn:\n",
    "        for i in [0,1,2,3]:\n",
    "            img_dict[i] = get_img_dict_attn(df_mod = df_mod[i], model_path = model_path[i], model = model[i], config = config[i], base_imgs = base_imgs[i], regen = regen)\n",
    "    else:\n",
    "        for i in [0,1,2,3]:\n",
    "            img_dict[i] = get_img_dict(df_mod = df_mod[i], model_path = model_path[i], model = model[i], config = config[i], base_imgs = base_imgs[i], regen = regen)\n",
    "\n",
    "    col = 0\n",
    "    select_rows = [1, 3, 5]\n",
    "    axs = subfig.subplots(rows, len(select_rows))\n",
    "    for img_name in img_dict:\n",
    "        if col == 0 or col == 2:\n",
    "            select_rows = [0, 3, 6]\n",
    "        else:\n",
    "            select_rows = [1, 3, 5]\n",
    "        for row_num in range(len(select_rows)):\n",
    "            row = select_rows[row_num]\n",
    "            axs[col][row_num].imshow(img_dict[img_name][row])\n",
    "            #axs[col][row].spines['right'].set_visible(False)\n",
    "            #axs[col][row].spines['top'].set_visible(False)\n",
    "            #axs[col][row].spines['left'].set_visible(False)\n",
    "            #axs[col][row].spines['bottom'].set_visible(False)\n",
    "            axs[col][row_num].set_xticks([])\n",
    "            axs[col][row_num].set_yticks([])\n",
    "            axs[col][row_num].xaxis.set_label_position('top')\n",
    "        axs[col][0].set_ylabel(datasets[col], size = 18)\n",
    "        col = col + 1\n",
    "    count = 0\n",
    "    subfig.subplots_adjust(left = 0 + 1/24, top = 1, right = 1 - 1/24, bottom = 0 + 2/16, wspace=0, hspace=0)\n",
    "    padding = 3\n",
    "    axs[0][0].annotate(text=\"\", xy=(0.92, 1.032), xytext=(0.505,1.032), xycoords=\"subfigure fraction\",  arrowprops=dict(facecolor='C1'))\n",
    "    axs[0][0].annotate(text=\"\", xy=(0.08, 1.032), xytext=(0.495,1.032), xycoords=\"subfigure fraction\",  arrowprops=dict(facecolor='C0'))\n",
    "    axs[0][0].annotate(text=label[0], xy = (0.50,1.05), xycoords=\"subfigure fraction\", ha=\"center\", size = 18)\n",
    "    axs[0][0].annotate(text=label[1], xy = (0.08, 1.05), xycoords=\"subfigure fraction\", ha=\"center\", size = 16)\n",
    "    axs[0][0].annotate(text=label[2], xy = (0.92, 1.05), xycoords=\"subfigure fraction\", ha=\"center\", size = 16)\n",
    "    if label[0] == 'Subtype Prediction':\n",
    "        axs[0][0].annotate(text=\"Ductal\", xy = (0.0, 1.0), xytext=(padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"left\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[0][len(select_rows) - 1].annotate(text=\"Lobular\", xy = (1.0, 1.0), xytext=(-padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"right\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[1][0].annotate(text=\"Adeno\", xy = (0.0, 1.0), xytext=(padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"left\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[1][len(select_rows) - 1].annotate(text=\"Squamous\", xy = (1.0, 1.0), xytext=(-padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"right\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[2][0].annotate(text=\"Adeno\", xy = (0.0, 1.0), xytext=(padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"left\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[2][len(select_rows) - 1].annotate(text=\"Squamous\", xy = (1.0, 1.0), xytext=(-padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"right\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[3][0].annotate(text=\"Clear\", xy = (0.0, 1.0), xytext=(padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"left\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "        axs[3][len(select_rows) - 1].annotate(text=\"Papillary\", xy = (1.0, 1.0),xytext=(-padding, -padding), xycoords=\"axes fraction\", textcoords='offset points', ha=\"right\", va = \"top\", size = 16,  bbox=dict(pad = padding, facecolor='white', edgecolor='black'))\n",
    "\n",
    "    subfig.text(-0.01,1.02,label[3], zorder = 30, clip_on = False, weight='bold', size = 20 )\n",
    "    \n",
    "\n",
    "    \n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "base_imgs =  [100, 200, 401, 501]\n",
    "fig_overall = plt.figure(figsize=(13, 18), dpi = 300)\n",
    "subfigs = fig_overall.subfigures(2, 2)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "df_mod = []\n",
    "datasets = ['BRCA', 'PAAD', 'HNSC', 'PRAD']\n",
    "\n",
    "for d in datasets:\n",
    "    df_mod += [pd.read_csv(PROJECT_DIR + '/PROJECTS/HistoXGAN/SAVED_FEATURES/' + d.lower() + '_features_part.csv')]\n",
    "\n",
    "model_paths = []\n",
    "for d in datasets:\n",
    "    model_paths += [PROJECT_DIR + '/PRETRAINED_MODELS/' + d  + '_GRADE_MIL/']\n",
    "models = []\n",
    "configs = []\n",
    "\n",
    "for m in model_paths:\n",
    "    model, config = load_model_weights(m)\n",
    "    model.eval().to(device)\n",
    "    models += [model]\n",
    "    configs += [config]\n",
    "\n",
    "base_imgs = [100, 100, 200, 200]\n",
    "\n",
    "plt_imgs(df_mod, model_paths, models, configs, base_imgs, regen = True, attn = False, subfig = subfigs[0][0], label = ['Grade Prediction', 'Low', 'High', 'A'],datasets = datasets)\n",
    "plt_imgs(df_mod, model_paths, models, configs, base_imgs, regen = True, attn = True, subfig = subfigs[0][1], label = ['Grade Attention', 'Low', 'High', 'B'], datasets = datasets)\n",
    "\n",
    "base_imgs = [100, 100, 200, 6]\n",
    "df_mod = []\n",
    "datasets = ['BRCA', 'LUNG', 'ESCA', 'KIDNEY']\n",
    "\n",
    "for d in datasets:\n",
    "    dset = d\n",
    "    if dset == 'LUNG':\n",
    "        dset = 'LUAD'\n",
    "    if dset == 'KIDNEY':\n",
    "        dset = 'KIRP'\n",
    "    df_mod += [pd.read_csv(PROJECT_DIR + '/PROJECTS/HistoXGAN/SAVED_FEATURES/' + dset.lower() + '_features_part.csv')]\n",
    "\n",
    "model_paths = []\n",
    "for d in datasets:\n",
    "    model_paths += [PROJECT_DIR + '/PRETRAINED_MODELS/' + d  + '_SUBTYPE_MIL/']\n",
    "models = []\n",
    "configs = []\n",
    "\n",
    "for m in model_paths:\n",
    "    model, config = load_model_weights(m)\n",
    "    model.eval().to(device)\n",
    "    models += [model]\n",
    "    configs += [config]\n",
    "\n",
    "base_imgs = [100, 100, 100, 100]\n",
    "plt_imgs(df_mod, model_paths, models, configs, base_imgs, regen = True, attn = False, subfig = subfigs[1][0], label = ['Subtype Prediction', '', '', 'C'], datasets = datasets)\n",
    "plt_imgs(df_mod, model_paths, models, configs, base_imgs, regen = True, attn = True, subfig = subfigs[1][1], label = ['Subtype Attention', 'Low', 'High', 'D'], datasets = datasets)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed16f83",
   "metadata": {},
   "source": [
    "# Generation of Features from MRI Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc4a4a",
   "metadata": {},
   "source": [
    "### Loading extracted mean feature vectors and MRI radiomic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28890ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataframes trained / test split with mean ctranspath features per slide as well as MRI radiomic features\n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/dsf_stats_testk.pkl\", 'rb') as f:\n",
    "    dsf_stats_testk = pickle.load(f)\n",
    "\n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/dsf_stats_traink.pkl\", 'rb') as f:\n",
    "    dsf_stats_traink = pickle.load(f)\n",
    "\n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/feat_cols.pkl\", 'rb') as f:\n",
    "    feat_cols = pickle.load(f)\n",
    "\n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/feat_cols_pca.pkl\", 'rb') as f:\n",
    "    feat_cols_pca = pickle.load(f)\n",
    "    \n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/df_composite_corr.pkl\", 'rb') as f:\n",
    "    df_composite_corr = pickle.load(f)\n",
    "\n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/df_corr.pkl\", 'rb') as f:\n",
    "    df_corr = pickle.load(f)\n",
    "\n",
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/img_reals.pkl\", 'rb') as f:\n",
    "    img_reals = pickle.load(f)\n",
    "\n",
    "list_sigs = dsf_stats_testk[0].columns.tolist()[-777:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2091883",
   "metadata": {},
   "source": [
    "### Train simple encoder to convert from MRI features to CTransPath features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(feat_cols)\n",
    "encoded_dim = 768\n",
    "from slideflow.gan.stylegan3.stylegan3 import dnnlib, legacy, utils\n",
    "with dnnlib.util.open_url(PROJECT_DIR + '/FINAL_MODELS/CTransPath/snapshot.pkl') as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "    \n",
    "    \n",
    "from slideflow.model import build_feature_extractor\n",
    "ctranspath = build_feature_extractor('ctranspath', tile_px=512, device = device, force_uint8 = False, no_grad = False)\n",
    "for i in [0,1,2,3,4]:\n",
    "    encoder = torch.nn.Sequential(\n",
    "                          torch.nn.Linear(input_dim, encoded_dim),\n",
    "                          torch.nn.LeakyReLU(),\n",
    "                          torch.nn.Linear(encoded_dim, encoded_dim)\n",
    "    ).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-4, weight_decay = 1e-6)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        loss_total = 0\n",
    "        loss_steps = 0\n",
    "        for chunk in np.array_split(dsf_stats_traink[i], len(dsf_stats_traink[i].index)/16):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            source = torch.tensor(chunk[feat_cols].values).to(device).to(torch.float32)\n",
    "            target = torch.tensor(chunk.ctranspath_mean.apply(pd.Series).values).to(device).to(torch.float32)\n",
    "            \n",
    "            e = encoder(source.requires_grad_())\n",
    "            e2 = ctranspath(((G(e, 0, noise_mode = 'const') + 1)*127.5).requires_grad_())\n",
    "            loss = loss_fn(e, target) + loss_fn(e2, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    encoder.eval()\n",
    "    ev = encoder(torch.tensor(dsf_stats_testk[i][feat_cols].values).to(device).to(torch.float32))\n",
    "    dsf_stats_testk[i]['ctranspath_pred'] = [*(ev).detach().cpu().numpy()]\n",
    "    dsf_stats_testk[i][\"ctranspath_pred_regen\"] = dsf_stats_testk[i][\"ctranspath_pred\"].apply(lambda x: ctranspath(((G(torch.unsqueeze(torch.tensor(x).to(device), dim= 0), 0, noise_mode = 'const') + 1)*127.5).clamp(0, 255).to(torch.uint8))[0].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae5507",
   "metadata": {},
   "source": [
    "### Calculate statistics from trained encoder results\n",
    "Note - this requires all 777 trained SSL models for prediction of expression signatures, grade, and subtype to be present in the appropriate subfolder. We have provided pretrained versions of these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "sf.util.setLoggingLevel(50)\n",
    "\n",
    "dsf_stats_merge = pd.concat(dsf_stats_testk)\n",
    "for c in list_sigs: \n",
    "    if c != 'patient' and c != 'slide':\n",
    "        model_name = f'attention_mil-{c}'\n",
    "        import os\n",
    "        matching = [\n",
    "            o for o in os.listdir(PROJECT_DIR + \"/PRETRAINED_MODELS/mil/\")\n",
    "            if o == model_name\n",
    "        ]\n",
    "        if c != 'slide' and c != 'patient' and len(matching) >= 1:\n",
    "            model_name = PROJECT_DIR + \"/PRETRAINED_MODELS/mil/\" +  matching[len(matching)-1]\n",
    "                                                \n",
    "            model, config = load_model_weights(model_name)\n",
    "            model.eval().to(device)\n",
    "            \n",
    "            dsf_stats_merge[c + \"_pred\"] = dsf_stats_merge['ctranspath_pred_regen'].apply(lambda x: run_inference(model, torch.unsqueeze(torch.unsqueeze(torch.tensor(x).to(device), dim= 0), dim= 0), attention = False, use_lens = config.model_config.use_lens)[0].detach().cpu().numpy()[0][1])\n",
    "            dsf_stats_merge[c + \"_st\"] = dsf_stats_merge['ctranspath_mean'].apply(lambda x: run_inference(model, torch.unsqueeze(torch.unsqueeze(torch.tensor(x).to(device), dim= 0), dim= 0), attention = False, use_lens = config.model_config.use_lens)[0].detach().cpu().numpy()[0][1])\n",
    "    \n",
    "list_corr = []\n",
    "for c in list_sigs:\n",
    "    r, p = spearmanr(dsf_stats_merge[c + \"_st\"], dsf_stats_merge[c + \"_pred\"])\n",
    "    list_corr += [[c, r, p]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf4228",
   "metadata": {},
   "source": [
    "### Predictions from MRI using Logistic Regression to Directly Predict Expression Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Train logistic regression with a fit of MRI from PCA for overoptimisitc estimate of performance for a 'upper bound' on performance from reconstructed histology\n",
    "pred = None\n",
    "truevals = None\n",
    "y_preds_full = []\n",
    "y_trues_full = []\n",
    "for c in list_sigs:\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "\n",
    "    for k in [0,1,2,3,4]:\n",
    "        X_train = dsf_stats_traink[k][feat_cols_pca]\n",
    "        Y_train = dsf_stats_traink[k][c] > 0.5\n",
    "        clf = LogisticRegression().fit(X_train, Y_train)\n",
    "        y_preds += clf.predict_proba(dsf_stats_testk[k][feat_cols_pca])[:, 1].tolist()\n",
    "        y_trues += dsf_stats_testk[k][c].tolist()\n",
    "    #print(y_preds)\n",
    "    #print(y_trues)\n",
    "    y_preds_full += [y_preds]\n",
    "    y_trues_full += [y_trues]\n",
    "    \n",
    "list_corr2 = []\n",
    "for i in range(len(list_sigs)):\n",
    "    r, p = pearsonr(y_preds_full[i], y_trues_full[i])\n",
    "    list_corr2 += [[c, r, p]]\n",
    "    \n",
    "list_comb = []\n",
    "for row1, row2 in zip(list_corr, list_corr2):\n",
    "    list_comb += [[row1[0], row1[1], row1[2], row2[1], row2[2]]]\n",
    "list_comb = np.array(list_comb)\n",
    "\n",
    "df_corr = pd.DataFrame(list_comb, columns = ['signature_name', 'gen_r', 'gen_p', 'mri_r', 'mri_p'])\n",
    "from statsmodels.stats.multitest import fdrcorrection \n",
    "df_corr['gen_p_fdr'] = fdrcorrection(df_corr['gen_p'])[1]\n",
    "df_corr['mri_p_fdr'] = fdrcorrection(df_corr['mri_p'])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e020696",
   "metadata": {},
   "source": [
    "### Can load the results of pretrained encoder here for replication of publication statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d4b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROJECT_DIR + \"/pub_pkl/mri_recreation/dsf_stats_merge.pkl\", 'rb') as f:\n",
    "    dsf_stats_merge = pickle.load(f)\n",
    "    \n",
    "with open(PROJECT_DIR + \"/pub_pkl/sig_clusters.pkl\", 'rb') as f:\n",
    "    saved_clusters = pickle.load(f)\n",
    "\n",
    "with open(PROJECT_DIR + \"/pub_pkl/xind.pkl\", 'rb') as f:\n",
    "    xind = pickle.load(f)\n",
    "\n",
    "xind.insert(180, 775)\n",
    "xind.insert(345, 776)\n",
    "saved_clusters = np.concatenate([saved_clusters, [2, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7dc0b6",
   "metadata": {},
   "source": [
    "### Plot figure of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import cv2\n",
    "\n",
    "#feature_links = scipy.cluster.hierarchy.linkage(df_composite_corr, method='ward', metric='euclidean')\n",
    "#patient_links = scipy.cluster.hierarchy.linkage(df_composite_corr.transpose(), method='ward', metric='euclidean')\n",
    "\n",
    "\n",
    "from slideflow.gan.stylegan3.stylegan3 import dnnlib, legacy, utils\n",
    "with dnnlib.util.open_url(PROJECT_DIR + '/FINAL_MODELS/CTransPath/snapshot.pkl') as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device)\n",
    "\n",
    "fig_overall = plt.figure(figsize=(14, 14), dpi = 300)\n",
    "subfigs_overall = fig_overall.subfigures(2, 1)\n",
    "\n",
    "plot_list = [304, 274, 906, 298] #[304, 274, 906, 475, 298, 481, 378]\n",
    "\n",
    "subfigs_top = subfigs_overall[0].subfigures(1,2, width_ratios = [6, 8])\n",
    "ax = subfigs_top[0].subplots(1,1)\n",
    "\n",
    "im = cv2.imread(PROJECT_DIR + '/FINAL_IMAGES/mripath.png')\n",
    "\n",
    "ax.imshow(cv2.cvtColor(cv2.resize(im, (1285, 1279), interpolation=cv2.INTER_LINEAR), cv2.COLOR_BGR2RGB))\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "for side in ['top','right','bottom','left']:\n",
    "    ax.spines[side].set_visible(False)\n",
    "axs = subfigs_top[1].subplots(3, 4)\n",
    "\n",
    "\n",
    "#imgs_plot = {'mean':{}, 'pred':{}}\n",
    "#for i in range(len(dsf_stats_merge.index)):\n",
    "#    with torch.no_grad():\n",
    "#        imgs_plot['pred'][i] = ((G(torch.tensor([dsf_stats_merge.iloc[i]['ctranspath_pred']]).to(device), 0, noise_mode ='const') + 1)*127.5).permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "#        imgs_plot['mean'][i] = ((G(torch.tensor([dsf_stats_merge.iloc[i]['ctranspath_mean']]).to(device), 0, noise_mode ='const') + 1)*127.5).permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "for i in range(len(plot_list)):\n",
    "    axs[0][i].imshow(img_reals[plot_list[i]])\n",
    "    axs[1][i].imshow(imgs_plot['mean'][plot_list[i]])\n",
    "    axs[2][i].imshow(imgs_plot['pred'][plot_list[i]])\n",
    "    axs[0][i].set_xticks([])\n",
    "    axs[0][i].set_yticks([])\n",
    "    axs[1][i].set_xticks([])\n",
    "    axs[1][i].set_yticks([])\n",
    "    axs[2][i].set_xticks([])\n",
    "    axs[2][i].set_yticks([])\n",
    "axs[0][0].set_ylabel(\"Tile from Slide\")\n",
    "axs[1][0].set_ylabel(\"Generated from Avg. Features\")\n",
    "axs[2][0].set_ylabel(\"Generated from MRI\")\n",
    "\n",
    "\n",
    "\n",
    "subfigs_top[0].subplots_adjust(left = 0, right = 0.9, top = 1, bottom = 0.15, wspace = 0, hspace = 0)\n",
    "\n",
    "subfigs_top[1].subplots_adjust(left = 0, right = 1, top = 1, bottom = 0.15, wspace = 0, hspace = 0)\n",
    "\n",
    "\n",
    "subfigs = subfigs_overall[1].subfigures(1, 3, width_ratios = [2, 7, 5])\n",
    "\n",
    "ax = subfigs[0].subplots()\n",
    "labels = [\"Mean Inter-Tumor\\nFeature Difference\", \"Mean Feature Difference\\nVirtual Biopsy vs Digital Slide\", \"Mean Intra-Tumor\\nFeature Difference\"]\n",
    "cmap = matplotlib.cm.get_cmap('Blues')\n",
    "norm = matplotlib.colors.Normalize(vmin=0, vmax = 5)\n",
    "\n",
    "ax.bar(0, 0.074, edgecolor = 'black', color = cmap(norm(1)))\n",
    "ax.errorbar(0, 0.074, yerr=0.001, fmt='none', color='black', capsize=3)\n",
    "ax.bar(1, 0.078, edgecolor = 'black', color = cmap(norm(3)))\n",
    "ax.errorbar(1, 0.078, yerr=0.001, fmt='none', color='black', capsize=3)\n",
    "ax.bar(2, 0.092, edgecolor = 'black', color = cmap(norm(5)))\n",
    "ax.errorbar(2, 0.092, yerr=0.001, fmt='none', color='black', capsize=3)\n",
    "ax.set_ylim(0.05, 0.10)\n",
    "ax.set_ylabel(\"Mean Absolute Difference\")\n",
    "\n",
    "ax.set_xticks([0, 1, 2])\n",
    "ax.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "subfigs[0].subplots_adjust(left = 0, right = 0.8, top = 1.0, bottom = 2/5, wspace = 0.1, hspace = 0.1)\n",
    "\n",
    "ax = subfigs[2].subplots()\n",
    "\n",
    "import scipy.cluster.hierarchy as hac\n",
    "z = hac.linkage(df_composite_corr, method='ward', metric='euclidean')\n",
    "clust_num = 5\n",
    "part1 = hac.fcluster(z, clust_num, 'maxclust')\n",
    "from matplotlib.lines import Line2D\n",
    "ax.axhline(y = 0.071, color='C0', linestyle='dashed', alpha = 0.5)\n",
    "ax.axvline(x = 0.081, color='C0', linestyle='dashed', alpha = 0.5, label = 'Significance Threshold (FDR corrected)')\n",
    "\n",
    "for i in range(clust_num):\n",
    "    #ax.scatter(df_corr['gen_r'].iloc[part1 == i + 1], df_corr['mri_r'].iloc[part1 == i + 1], color = cmap(norm(i + 1)))\n",
    "    ax.scatter(df_corr['gen_r'].iloc[saved_clusters == i + 1], df_corr['mri_r'].iloc[saved_clusters == i + 1], color = cmap(norm(i + 1)))\n",
    "\n",
    "#(PMID 24516633)\n",
    "ax.annotate(\"IFN3\", xy = (df_corr['gen_r'].iloc[190], df_corr['mri_r'].iloc[190]),\n",
    "                   xytext = (df_corr['gen_r'].iloc[190] - 0.01, df_corr['mri_r'].iloc[190] - 0.02), bbox=dict(boxstyle='square,pad=0.01', fc='none', ec='none'),\n",
    "                       arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\", relpos=(0.5,1)))\n",
    "# (PMID34711841)\n",
    "ax.annotate(\"Trop2\", xy = (df_corr['gen_r'].iloc[679], df_corr['mri_r'].iloc[679]),\n",
    "                   xytext = (df_corr['gen_r'].iloc[679], df_corr['mri_r'].iloc[679] + 0.015), bbox=dict(boxstyle='square,pad=0.01', fc='none', ec='none'),\n",
    "                       arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\", relpos=(0.5,0)))\n",
    "#(PMID 15591335)\n",
    "ax.annotate(\"Oncotype\", xy = (df_corr['gen_r'].iloc[670], df_corr['mri_r'].iloc[670]),  \n",
    "                                        xytext = (df_corr['gen_r'].iloc[670] + 0.02, df_corr['mri_r'].iloc[670] - 0.02), bbox=dict(boxstyle='square,pad=0.01', fc='none', ec='none'),\n",
    "            arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\", relpos=(0.1,0.8)))\n",
    "#(PMID 19204204)\n",
    "ax.annotate(\"Prosigna\", xy = (df_corr['gen_r'].iloc[666], df_corr['mri_r'].iloc[666]),\n",
    "                                        xytext = (df_corr['gen_r'].iloc[666] + 0.01, df_corr['mri_r'].iloc[666] + 0.02), bbox=dict(boxstyle='square,pad=0.01', fc='none', ec='none'),\n",
    "            \n",
    "            arrowprops=dict(arrowstyle=\"->\",\n",
    "                            connectionstyle=\"arc3\", relpos=(0.1,0.2)))\n",
    "legs = []\n",
    "for c, i in zip([\"Hypoxia / Angiogensis\", \"Immune\", \"Basal\", \"Luminal\", \"Other\"], [5,4,3,2,1]):\n",
    "    legs += [Line2D([0], [0], marker = 'o', color = 'w', markerfacecolor = cmap(norm(i)), markersize = 8, label = c)]\n",
    "legs += [Line2D([0], [0], color='C0', linestyle = 'dashed', alpha = 0.5, label='Significant (FDR corrected)')]\n",
    "                       \n",
    "leg = ax.legend(handles=legs, loc='lower right', framealpha=1)\n",
    "for lh in leg.legendHandles:\n",
    "    lh.set_alpha(1)\n",
    "\n",
    "#ax.fill([0.081, 0.081, 0.25, 0.25], [0.071, 0.4, 0.4, 0.071], color = 'C0', alpha = 0.2, label = \"Signficant with FDR corr.\")\n",
    "#ax.legend()\n",
    "ax.set_xlabel(\"Pearson r, Prediction from Generated Histology\")\n",
    "ax.set_ylabel(\"Pearson r, Prediction from MRI\")\n",
    "#ax.set_xlim([-0.1,0.3])\n",
    "subfigs[2].subplots_adjust(left = 0, right = 1, top = 1.0, bottom = +1/5, wspace = 0.1, hspace = 0.1)\n",
    "\n",
    "axs = subfigs[1].subplots(2, 2, height_ratios = [8, 0.5], width_ratios = [8, 0.5])\n",
    "#col_dendrogram = scipy.cluster.hierarchy.dendrogram(feature_links, no_plot=True)\n",
    "#row_dendrogram = scipy.cluster.hierarchy.dendrogram(patient_links, no_plot=True)\n",
    "\n",
    "#col_dendrogram = scipy.cluster.hierarchy.dendrogram(feature_links, ax=ax_col_dendrogram)\n",
    "#row_dendrogram = scipy.cluster.hierarchy.dendrogram(patient_links, no_plot=True)\n",
    "#ax_col_dendrogram.set_axis_off()\n",
    "\n",
    "#xind = col_dendrogram['leaves']\n",
    "#yind = row_dendrogram['leaves']\n",
    "data = pd.DataFrame(df_composite_corr)\n",
    "#cm = axs[0][0].imshow(data.iloc[xind,yind].T, cmap='magma', vmin = -0.9, vmax = 0.9)\n",
    "cm = axs[0][0].imshow(data.iloc[xind,xind].T, cmap='magma', vmin = -1, vmax = 1)\n",
    "\n",
    "\n",
    "cbar = subfigs[1].colorbar(cm, cax = axs[0][1], shrink = 0.5)\n",
    "#axs[0][1].set_xticks([0,1])\n",
    "#axs[0][1].set_yticks([0,1])\n",
    "\n",
    "#axs[0][1].annotate(\"1\", (0.5, 1.0),  clip_on = False)\n",
    "#axs[0][1].annotate(\"-1\", (0.5, 0),  clip_on = False,)\n",
    "\n",
    "cbar.set_label('Pearson Correlation', rotation=90, zorder = 100, labelpad = -10)\n",
    "cbar.ax.set_yticks([-1, 1])\n",
    "cbar.ax.set_yticklabels(['-1', '1'], zorder = 100)\n",
    "#axs[0][1].set_ylabel('Pearson Correlation', clip_on = False)\n",
    "#axs[0][1].set_yticks([-0.8, -0.4, 0.4, 0.8])\n",
    "\n",
    "axs[0][0].set_title(\"Correlation Matrix of Predicted Expression Signatures\")\n",
    "axs[0][0].set_xticks([])\n",
    "axs[0][0].set_yticks([])\n",
    "newcmp = ListedColormap([\"C0\",\"C1\",\"C2\",\"C3\",\"C4\"], name='RedBlue')\n",
    "\n",
    "\n",
    "#axs[1][0].imshow(np.vstack([part1[xind],part1[xind]]), extent=[0,len(part1),0,1], aspect = 'auto', cmap='Blues', vmin = 0, vmax = 5)\n",
    "axs[1][0].imshow(np.vstack([saved_clusters[xind],saved_clusters[xind]]), extent=[0,len(saved_clusters),0,1], aspect = 'auto', cmap='Blues', vmin = 0, vmax = 5)\n",
    "tick_locs = []\n",
    "tick_a = 0\n",
    "for i in range(5):\n",
    "    #tick_locs += [tick_a + sum(part1 == (i + 1)) / 2]\n",
    "    #tick_a += sum(part1 == (i + 1))\n",
    "    tick_locs += [tick_a + sum(saved_clusters == (i + 1)) / 2]\n",
    "    tick_a += sum(saved_clusters == (i + 1))\n",
    "\n",
    "axs[1][0].set_xticks(tick_locs)\n",
    "#axs[1][0].set_xticklabels([\"Luminal\", \"Stroma\", \"Immune\", \"IFN / Basal\", \"Other\"])\n",
    "axs[1][0].set_xticklabels([\"Other\", \"Luminal\", \"Basal\", \"Immune\", \"Hypoxia / Angiogensis\"])\n",
    "axs[1][0].get_yaxis().set_visible(False)\n",
    "\n",
    "axs[0][1].get_yaxis().set_visible(True)\n",
    "subfigs[1].subplots_adjust(left = 0, right = 0.8, top = 1.0, bottom = +1/5, wspace = 0.1, hspace = 0.1)\n",
    "subfigs[1].delaxes(axs[1][1])\n",
    "\n",
    "\n",
    "#axs[1][1].get_xaxis().set_visible(False)\n",
    "#axs[1][1].get_yaxis().set_visible(False)\n",
    "\n",
    "subfigs_overall[0].text(-0.03,1.02, \"A\", zorder = 30, clip_on = False, weight='bold', size = 20 )\n",
    "subfigs[0].text(-0.13,1.02, \"B\", zorder = 30, clip_on = False, weight='bold', size = 20 )\n",
    "subfigs[1].text(-0.03,1.02, \"C\", zorder = 30, clip_on = False, weight='bold', size = 20 )\n",
    "subfigs[2].text(-0.03 ,1.02, \"D\", zorder = 30, clip_on = False, weight='bold', size = 20 )\n",
    "#plt.savefig(\"/mnt/data/fred/figure_6.png\")\n",
    "plt.show()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5874e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
